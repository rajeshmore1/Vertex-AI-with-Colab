{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Serving Machine Learning models with Google Vertex AI\n",
        "\n",
        "Great to have you here, this is the code for the following article:\n",
        "\n",
        "* https://medium.com/google-cloud/serving-machine-learning-models-with-google-vertex-ai-5d9644ededa3\n",
        "\n",
        "Your feedback and questions are highly appreciated. <br>You can find me on Twitter [@HeyerSascha](https://twitter.com/HeyerSascha) or connect with me via [LinkedIn](https://www.linkedin.com/in/saschaheyer/). <br>Even better, subscribe to my [YouTube](https://www.youtube.com/channel/UC--Sm3D-rqCUeLXmraypdPQ) channel ❤️."
      ],
      "metadata": {
        "id": "sMWhF8u7wLze"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJH4O6qUBEPQ"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9l9QSaRk09q",
        "outputId": "98777aee-689f-4876-faa2-d701b64db89d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n"
          ]
        }
      ],
      "source": [
        "! gcloud config set project sascha-playground-doit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7oubOpffyzr"
      },
      "source": [
        "## Custom Prediction Container with FastAPI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZE6YY_8xFwsC",
        "outputId": "8871b76f-6077-46d7-d144-bbc061edfd40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting main.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile main.py\n",
        "import uvicorn\n",
        "\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "from enum import Enum\n",
        "from typing import List, Optional\n",
        "from pydantic import BaseModel\n",
        "\n",
        "from fastapi import Request, FastAPI, Response\n",
        "from fastapi.responses import JSONResponse\n",
        "from transformers import DistilBertTokenizerFast\n",
        "from transformers import TFDistilBertForSequenceClassification\n",
        "\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
        "model = TFDistilBertForSequenceClassification.from_pretrained(\"../sentiment\")\n",
        "\n",
        "app = FastAPI(title=\"Sentiment Analysis\")\n",
        "\n",
        "AIP_HEALTH_ROUTE = os.environ.get('AIP_HEALTH_ROUTE', '/health')\n",
        "AIP_PREDICT_ROUTE = os.environ.get('AIP_PREDICT_ROUTE', '/predict')\n",
        "\n",
        "class Prediction(BaseModel):\n",
        "  sentiment: str \n",
        "  confidence: Optional[float]\n",
        "\n",
        "class Predictions(BaseModel):\n",
        "    predictions: List[Prediction]\n",
        "\n",
        "# instad of creating a class we could have also loaded this information\n",
        "# from the model configuration. Better if you introduce new labels over time\n",
        "class Sentiment(Enum):\n",
        "  NEGATIVE = 0\n",
        "  POSITIVE = 1\n",
        "\n",
        "\n",
        "@app.get(AIP_HEALTH_ROUTE, status_code=200)\n",
        "async def health():\n",
        "    return {'health': 'ok'}\n",
        "\n",
        "@app.post(AIP_PREDICT_ROUTE, \n",
        "          response_model=Predictions,\n",
        "          response_model_exclude_unset=True)\n",
        "async def predict(request: Request):\n",
        "    body = await request.json()\n",
        "    print(body)\n",
        "\n",
        "    instances = body[\"instances\"]\n",
        "    print(instances)\n",
        "    print(type(instances))\n",
        "    instances = [x['text'] for x in instances]\n",
        "    print(instances)\n",
        "\n",
        "    tf_batch = tokenizer(instances, max_length=128, padding=True,\n",
        "                            truncation=True, return_tensors='tf')\n",
        "\n",
        "    print(tf_batch)\n",
        "\n",
        "    tf_outputs = model(tf_batch)\n",
        "\n",
        "    print(tf_outputs)\n",
        "\n",
        "    tf_predictions = tf.nn.softmax(tf_outputs[0], axis=-1)\n",
        "    print(tf_predictions)\n",
        "\n",
        "    indices = np.argmax(tf_predictions, axis=-1)\n",
        "    confidences = np.max(tf_predictions, axis=-1)\n",
        "\n",
        "    outputs = []\n",
        "\n",
        "    for index, confidence in zip(indices, confidences):\n",
        "      sentiment = Sentiment(index).name\n",
        "      print(index)\n",
        "      print(confidence)\n",
        "      outputs.append(Prediction(sentiment=sentiment, confidence=confidence))\n",
        "\n",
        "    return Predictions(predictions=outputs)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  app.run(debug=True, host=\"0.0.0.0\",port=8080)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eB6Ro6IFwAh",
        "outputId": "1a968b7d-0f45-4865-d236-734fab9facf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting Dockerfile\n"
          ]
        }
      ],
      "source": [
        "%%writefile Dockerfile\n",
        "FROM tiangolo/uvicorn-gunicorn-fastapi:python3.8-slim\n",
        "RUN pip install --no-cache-dir transformers==4.1.1 tensorflow==2.9.1 numpy==1.23.1 pydantic==1.9.1\n",
        "COPY main.py ./main.py\n",
        "COPY ./sentiment /sentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kIFMOMaF4aA",
        "outputId": "b09169ae-403a-451e-a750-d19681b25b51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing cloudbuild.yaml\n"
          ]
        }
      ],
      "source": [
        "%%writefile cloudbuild.yaml\n",
        "steps:\n",
        "# Download the model to embed it into the image\n",
        "- name: 'gcr.io/cloud-builders/gsutil'\n",
        "  args: ['cp','-r', 'gs://doit-vertex-demo/models/sentiment', '.']\n",
        "  id: 'download-model'\n",
        "# Build the container image\n",
        "- name: 'gcr.io/cloud-builders/docker'\n",
        "  args: ['build', '-t', 'gcr.io/sascha-playground-doit/sentiment-fast-api', '.']\n",
        "  waitFor: ['download-model']\n",
        "# Push the container image to Container Registry\n",
        "- name: 'gcr.io/cloud-builders/docker'\n",
        "  args: ['push', 'gcr.io/sascha-playground-doit/sentiment-fast-api']\n",
        "\n",
        "images:\n",
        "- gcr.io/sascha-playground-doit/sentiment-fast-api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ivofO1oGAXk",
        "outputId": "98fabdc0-bcce-480b-e0db-70bae28e721e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating temporary tarball archive of 25 file(s) totalling 54.2 MiB before compression.\n",
            "Uploading tarball of [.] to [gs://sascha-playground-doit_cloudbuild/source/1667389674.261614-ea5c988004d04af3b1e5504d1d35cd0d.tgz]\n",
            "Created [https://cloudbuild.googleapis.com/v1/projects/sascha-playground-doit/locations/global/builds/75c0fb6f-a11b-4552-926f-d6a5aab12ab2].\n",
            "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/75c0fb6f-a11b-4552-926f-d6a5aab12ab2?project=234439745674 ].\n",
            " REMOTE BUILD OUTPUT\n",
            "starting build \"75c0fb6f-a11b-4552-926f-d6a5aab12ab2\"\n",
            "\n",
            "FETCHSOURCE\n",
            "Fetching storage object: gs://sascha-playground-doit_cloudbuild/source/1667389674.261614-ea5c988004d04af3b1e5504d1d35cd0d.tgz#1667389685171779\n",
            "Copying gs://sascha-playground-doit_cloudbuild/source/1667389674.261614-ea5c988004d04af3b1e5504d1d35cd0d.tgz#1667389685171779...\n",
            "/ [1 files][  6.5 MiB/  6.5 MiB]                                                \n",
            "Operation completed over 1 objects/6.5 MiB.\n",
            "BUILD\n",
            "Starting Step #0 - \"download-model\"\n",
            "Step #0 - \"download-model\": Already have image (with digest): gcr.io/cloud-builders/gsutil\n",
            "Step #0 - \"download-model\": Copying gs://doit-vertex-demo/models/sentiment/config.json...\n",
            "Copying gs://doit-vertex-demo/models/sentiment/tf_model.h5...\n",
            "/ [2 files][255.5 MiB/255.5 MiB]                                                \n",
            "Step #0 - \"download-model\": Operation completed over 2 objects/255.5 MiB.\n",
            "Finished Step #0 - \"download-model\"\n",
            "Starting Step #1\n",
            "Step #1: Already have image (with digest): gcr.io/cloud-builders/docker\n",
            "Step #1: Sending build context to Docker daemon  324.9MB\n",
            "Step #1: Step 1/4 : FROM tiangolo/uvicorn-gunicorn-fastapi:python3.8-slim\n",
            "Step #1: python3.8-slim: Pulling from tiangolo/uvicorn-gunicorn-fastapi\n",
            "Step #1: bd897bb914af: Pulling fs layer\n",
            "Step #1: aee78d822213: Pulling fs layer\n",
            "Step #1: 6d9f6b5c1e71: Pulling fs layer\n",
            "Step #1: cf9f290bd6be: Pulling fs layer\n",
            "Step #1: 5e4b501cbda5: Pulling fs layer\n",
            "Step #1: 89975425112a: Pulling fs layer\n",
            "Step #1: 0ebd8a000090: Pulling fs layer\n",
            "Step #1: c289d5250a39: Pulling fs layer\n",
            "Step #1: cc89997b3f24: Pulling fs layer\n",
            "Step #1: 179541ead341: Pulling fs layer\n",
            "Step #1: c80d188fbf51: Pulling fs layer\n",
            "Step #1: 1ffed44d707c: Pulling fs layer\n",
            "Step #1: 951f363acb35: Pulling fs layer\n",
            "Step #1: 364d802677b1: Pulling fs layer\n",
            "Step #1: af0e3222d9c2: Pulling fs layer\n",
            "Step #1: 60807e1fd55f: Pulling fs layer\n",
            "Step #1: cf9f290bd6be: Waiting\n",
            "Step #1: 5e4b501cbda5: Waiting\n",
            "Step #1: 89975425112a: Waiting\n",
            "Step #1: 0ebd8a000090: Waiting\n",
            "Step #1: c289d5250a39: Waiting\n",
            "Step #1: cc89997b3f24: Waiting\n",
            "Step #1: 179541ead341: Waiting\n",
            "Step #1: c80d188fbf51: Waiting\n",
            "Step #1: 1ffed44d707c: Waiting\n",
            "Step #1: 951f363acb35: Waiting\n",
            "Step #1: 364d802677b1: Waiting\n",
            "Step #1: af0e3222d9c2: Waiting\n",
            "Step #1: 60807e1fd55f: Waiting\n",
            "Step #1: aee78d822213: Verifying Checksum\n",
            "Step #1: aee78d822213: Download complete\n",
            "Step #1: cf9f290bd6be: Verifying Checksum\n",
            "Step #1: cf9f290bd6be: Download complete\n",
            "Step #1: 5e4b501cbda5: Download complete\n",
            "Step #1: 6d9f6b5c1e71: Download complete\n",
            "Step #1: 89975425112a: Verifying Checksum\n",
            "Step #1: 89975425112a: Download complete\n",
            "Step #1: c289d5250a39: Verifying Checksum\n",
            "Step #1: c289d5250a39: Download complete\n",
            "Step #1: bd897bb914af: Verifying Checksum\n",
            "Step #1: bd897bb914af: Download complete\n",
            "Step #1: cc89997b3f24: Verifying Checksum\n",
            "Step #1: cc89997b3f24: Download complete\n",
            "Step #1: 0ebd8a000090: Verifying Checksum\n",
            "Step #1: 0ebd8a000090: Download complete\n",
            "Step #1: c80d188fbf51: Verifying Checksum\n",
            "Step #1: c80d188fbf51: Download complete\n",
            "Step #1: 179541ead341: Verifying Checksum\n",
            "Step #1: 179541ead341: Download complete\n",
            "Step #1: 1ffed44d707c: Download complete\n",
            "Step #1: 364d802677b1: Verifying Checksum\n",
            "Step #1: 364d802677b1: Download complete\n",
            "Step #1: 951f363acb35: Verifying Checksum\n",
            "Step #1: 951f363acb35: Download complete\n",
            "Step #1: 60807e1fd55f: Verifying Checksum\n",
            "Step #1: 60807e1fd55f: Download complete\n",
            "Step #1: af0e3222d9c2: Verifying Checksum\n",
            "Step #1: af0e3222d9c2: Download complete\n",
            "Step #1: bd897bb914af: Pull complete\n",
            "Step #1: aee78d822213: Pull complete\n",
            "Step #1: 6d9f6b5c1e71: Pull complete\n",
            "Step #1: cf9f290bd6be: Pull complete\n",
            "Step #1: 5e4b501cbda5: Pull complete\n",
            "Step #1: 89975425112a: Pull complete\n",
            "Step #1: 0ebd8a000090: Pull complete\n",
            "Step #1: c289d5250a39: Pull complete\n",
            "Step #1: cc89997b3f24: Pull complete\n",
            "Step #1: 179541ead341: Pull complete\n",
            "Step #1: c80d188fbf51: Pull complete\n",
            "Step #1: 1ffed44d707c: Pull complete\n",
            "Step #1: 951f363acb35: Pull complete\n",
            "Step #1: 364d802677b1: Pull complete\n",
            "Step #1: af0e3222d9c2: Pull complete\n",
            "Step #1: 60807e1fd55f: Pull complete\n",
            "Step #1: Digest: sha256:3eeb2e51a74c8325806050ea1249950c944b59e8b9641ea1f89d9d6becd68154\n",
            "Step #1: Status: Downloaded newer image for tiangolo/uvicorn-gunicorn-fastapi:python3.8-slim\n",
            "Step #1:  ---> 24a351926e6f\n",
            "Step #1: Step 2/4 : RUN pip install --no-cache-dir transformers==4.1.1 tensorflow==2.9.1 numpy==1.23.1 pydantic==1.9.1\n",
            "Step #1:  ---> Running in d3691566011a\n",
            "Step #1: Collecting transformers==4.1.1\n",
            "Step #1:   Downloading transformers-4.1.1-py3-none-any.whl (1.5 MB)\n",
            "Step #1: Collecting tensorflow==2.9.1\n",
            "Step #1:   Downloading tensorflow-2.9.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n",
            "Step #1: Collecting numpy==1.23.1\n",
            "Step #1:   Downloading numpy-1.23.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "Step #1: Collecting pydantic==1.9.1\n",
            "Step #1:   Downloading pydantic-1.9.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
            "Step #1: Collecting sacremoses\n",
            "Step #1:   Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "Step #1: Collecting filelock\n",
            "Step #1:   Downloading filelock-3.8.0-py3-none-any.whl (10 kB)\n",
            "Step #1: Collecting tqdm>=4.27\n",
            "Step #1:   Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
            "Step #1: Collecting packaging\n",
            "Step #1:   Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
            "Step #1: Collecting tokenizers==0.9.4\n",
            "Step #1:   Downloading tokenizers-0.9.4-cp38-cp38-manylinux2010_x86_64.whl (2.9 MB)\n",
            "Step #1: Collecting requests\n",
            "Step #1:   Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "Step #1: Collecting regex!=2019.12.17\n",
            "Step #1:   Downloading regex-2022.10.31-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (772 kB)\n",
            "Step #1: Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/site-packages (from tensorflow==2.9.1) (3.10.0.2)\n",
            "Step #1: Requirement already satisfied: setuptools in /usr/local/lib/python3.8/site-packages (from tensorflow==2.9.1) (57.5.0)\n",
            "Step #1: Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
            "Step #1:   Downloading tensorflow_io_gcs_filesystem-0.27.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
            "Step #1: Collecting tensorboard<2.10,>=2.9\n",
            "Step #1:   Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
            "Step #1: Collecting absl-py>=1.0.0\n",
            "Step #1:   Downloading absl_py-1.3.0-py3-none-any.whl (124 kB)\n",
            "Step #1: Collecting google-pasta>=0.1.1\n",
            "Step #1:   Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "Step #1: Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n",
            "Step #1:   Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
            "Step #1: Collecting grpcio<2.0,>=1.24.3\n",
            "Step #1:   Downloading grpcio-1.50.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "Step #1: Collecting gast<=0.4.0,>=0.2.1\n",
            "Step #1:   Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Step #1: Collecting six>=1.12.0\n",
            "Step #1:   Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Step #1: Collecting protobuf<3.20,>=3.9.2\n",
            "Step #1:   Downloading protobuf-3.19.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "Step #1: Collecting keras<2.10.0,>=2.9.0rc0\n",
            "Step #1:   Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
            "Step #1: Collecting h5py>=2.9.0\n",
            "Step #1:   Downloading h5py-3.7.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.5 MB)\n",
            "Step #1: Collecting wrapt>=1.11.0\n",
            "Step #1:   Downloading wrapt-1.14.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (81 kB)\n",
            "Step #1: Collecting keras-preprocessing>=1.1.1\n",
            "Step #1:   Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "Step #1: Collecting termcolor>=1.1.0\n",
            "Step #1:   Downloading termcolor-2.1.0-py3-none-any.whl (5.8 kB)\n",
            "Step #1: Collecting astunparse>=1.6.0\n",
            "Step #1:   Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Step #1: Collecting libclang>=13.0.0\n",
            "Step #1:   Downloading libclang-14.0.6-py2.py3-none-manylinux2010_x86_64.whl (14.1 MB)\n",
            "Step #1: Collecting flatbuffers<2,>=1.12\n",
            "Step #1:   Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "Step #1: Collecting opt-einsum>=2.3.2\n",
            "Step #1:   Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
            "Step #1: Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow==2.9.1) (0.37.0)\n",
            "Step #1: Collecting google-auth<3,>=1.6.3\n",
            "Step #1:   Downloading google_auth-2.14.0-py2.py3-none-any.whl (175 kB)\n",
            "Step #1: Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
            "Step #1:   Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "Step #1: Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
            "Step #1:   Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Step #1: Collecting tensorboard-plugin-wit>=1.6.0\n",
            "Step #1:   Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "Step #1: Collecting werkzeug>=1.0.1\n",
            "Step #1:   Downloading Werkzeug-2.2.2-py3-none-any.whl (232 kB)\n",
            "Step #1: Collecting markdown>=2.6.8\n",
            "Step #1:   Downloading Markdown-3.4.1-py3-none-any.whl (93 kB)\n",
            "Step #1: Collecting pyasn1-modules>=0.2.1\n",
            "Step #1:   Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
            "Step #1: Collecting cachetools<6.0,>=2.0.0\n",
            "Step #1:   Downloading cachetools-5.2.0-py3-none-any.whl (9.3 kB)\n",
            "Step #1: Collecting rsa<5,>=3.1.4\n",
            "Step #1:   Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
            "Step #1: Collecting requests-oauthlib>=0.7.0\n",
            "Step #1:   Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
            "Step #1: Collecting importlib-metadata>=4.4\n",
            "Step #1:   Downloading importlib_metadata-5.0.0-py3-none-any.whl (21 kB)\n",
            "Step #1: Collecting zipp>=0.5\n",
            "Step #1:   Downloading zipp-3.10.0-py3-none-any.whl (6.2 kB)\n",
            "Step #1: Collecting pyasn1<0.5.0,>=0.4.6\n",
            "Step #1:   Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
            "Step #1: Collecting idna<4,>=2.5\n",
            "Step #1:   Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
            "Step #1: Collecting certifi>=2017.4.17\n",
            "Step #1:   Downloading certifi-2022.9.24-py3-none-any.whl (161 kB)\n",
            "Step #1: Collecting charset-normalizer<3,>=2\n",
            "Step #1:   Downloading charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
            "Step #1: Collecting urllib3<1.27,>=1.21.1\n",
            "Step #1:   Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
            "Step #1: Collecting oauthlib>=3.0.0\n",
            "Step #1:   Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
            "Step #1: Collecting MarkupSafe>=2.1.1\n",
            "Step #1:   Downloading MarkupSafe-2.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Step #1: Collecting pyparsing!=3.0.5,>=2.0.2\n",
            "Step #1:   Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
            "Step #1: Requirement already satisfied: click in /usr/local/lib/python3.8/site-packages (from sacremoses->transformers==4.1.1) (8.0.1)\n",
            "Step #1: Collecting joblib\n",
            "Step #1:   Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
            "Step #1: Building wheels for collected packages: sacremoses\n",
            "Step #1:   Building wheel for sacremoses (setup.py): started\n",
            "Step #1:   Building wheel for sacremoses (setup.py): finished with status 'done'\n",
            "Step #1:   Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=0458ddc12cfbd1162a2ed4991e25b78a167a6b80652a2365bdc3abe250ddb5b3\n",
            "Step #1:   Stored in directory: /tmp/pip-ephem-wheel-cache-f3vw4ngc/wheels/82/ab/9b/c15899bf659ba74f623ac776e861cf2eb8608c1825ddec66a4\n",
            "Step #1: Successfully built sacremoses\n",
            "Step #1: Installing collected packages: urllib3, pyasn1, idna, charset-normalizer, certifi, zipp, six, rsa, requests, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, MarkupSafe, importlib-metadata, google-auth, werkzeug, tqdm, tensorboard-plugin-wit, tensorboard-data-server, regex, pyparsing, protobuf, numpy, markdown, joblib, grpcio, google-auth-oauthlib, absl-py, wrapt, tokenizers, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, sacremoses, packaging, opt-einsum, libclang, keras-preprocessing, keras, h5py, google-pasta, gast, flatbuffers, filelock, astunparse, transformers, tensorflow, pydantic\n",
            "Step #1:   Attempting uninstall: pydantic\n",
            "Step #1:     Found existing installation: pydantic 1.8.2\n",
            "Step #1:     Uninstalling pydantic-1.8.2:\n",
            "Step #1:       Successfully uninstalled pydantic-1.8.2\n",
            "Step #1: Successfully installed MarkupSafe-2.1.1 absl-py-1.3.0 astunparse-1.6.3 cachetools-5.2.0 certifi-2022.9.24 charset-normalizer-2.1.1 filelock-3.8.0 flatbuffers-1.12 gast-0.4.0 google-auth-2.14.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.50.0 h5py-3.7.0 idna-3.4 importlib-metadata-5.0.0 joblib-1.2.0 keras-2.9.0 keras-preprocessing-1.1.2 libclang-14.0.6 markdown-3.4.1 numpy-1.23.1 oauthlib-3.2.2 opt-einsum-3.3.0 packaging-21.3 protobuf-3.19.6 pyasn1-0.4.8 pyasn1-modules-0.2.8 pydantic-1.9.1 pyparsing-3.0.9 regex-2022.10.31 requests-2.28.1 requests-oauthlib-1.3.1 rsa-4.9 sacremoses-0.0.53 six-1.16.0 tensorboard-2.9.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.9.1 tensorflow-estimator-2.9.0 tensorflow-io-gcs-filesystem-0.27.0 termcolor-2.1.0 tokenizers-0.9.4 tqdm-4.64.1 transformers-4.1.1 urllib3-1.26.12 werkzeug-2.2.2 wrapt-1.14.1 zipp-3.10.0\n",
            "Step #1: \u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
            "Step #1: \u001b[0m\u001b[91mWARNING: You are using pip version 21.2.4; however, version 22.3 is available.\n",
            "Step #1: You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\n",
            "Step #1: \u001b[0mRemoving intermediate container d3691566011a\n",
            "Step #1:  ---> 2dc3303c9fa7\n",
            "Step #1: Step 3/4 : COPY main.py ./main.py\n",
            "Step #1:  ---> 181ce7954d85\n",
            "Step #1: Step 4/4 : COPY ./sentiment /sentiment\n",
            "Step #1:  ---> de45f0dafb59\n",
            "Step #1: Successfully built de45f0dafb59\n",
            "Step #1: Successfully tagged gcr.io/sascha-playground-doit/sentiment-fast-api:latest\n",
            "Finished Step #1\n",
            "Starting Step #2\n",
            "Step #2: Already have image (with digest): gcr.io/cloud-builders/docker\n",
            "Step #2: Using default tag: latest\n",
            "Step #2: The push refers to repository [gcr.io/sascha-playground-doit/sentiment-fast-api]\n",
            "Step #2: a34e40efc55d: Preparing\n",
            "Step #2: 1368b6b98396: Preparing\n",
            "Step #2: c0239f8b4c11: Preparing\n",
            "Step #2: 52da08f2bae6: Preparing\n",
            "Step #2: 8f574fb9904c: Preparing\n",
            "Step #2: ef1a1e69814e: Preparing\n",
            "Step #2: f91f7d702c35: Preparing\n",
            "Step #2: ab561809c50c: Preparing\n",
            "Step #2: 77a4ffc5c2e3: Preparing\n",
            "Step #2: 869f30255f10: Preparing\n",
            "Step #2: 2a36a760e140: Preparing\n",
            "Step #2: 53760c3513ea: Preparing\n",
            "Step #2: eacc6208b134: Preparing\n",
            "Step #2: c34ed7e37231: Preparing\n",
            "Step #2: fafe219c2252: Preparing\n",
            "Step #2: 3f7799709962: Preparing\n",
            "Step #2: 2b24f8c3593f: Preparing\n",
            "Step #2: 25061b98c6eb: Preparing\n",
            "Step #2: a548c9107c3a: Preparing\n",
            "Step #2: ef1a1e69814e: Waiting\n",
            "Step #2: f91f7d702c35: Waiting\n",
            "Step #2: ab561809c50c: Waiting\n",
            "Step #2: 77a4ffc5c2e3: Waiting\n",
            "Step #2: 869f30255f10: Waiting\n",
            "Step #2: 2a36a760e140: Waiting\n",
            "Step #2: 53760c3513ea: Waiting\n",
            "Step #2: eacc6208b134: Waiting\n",
            "Step #2: c34ed7e37231: Waiting\n",
            "Step #2: fafe219c2252: Waiting\n",
            "Step #2: 3f7799709962: Waiting\n",
            "Step #2: 2b24f8c3593f: Waiting\n",
            "Step #2: 25061b98c6eb: Waiting\n",
            "Step #2: a548c9107c3a: Waiting\n",
            "Step #2: 52da08f2bae6: Layer already exists\n",
            "Step #2: 8f574fb9904c: Layer already exists\n",
            "Step #2: ef1a1e69814e: Layer already exists\n",
            "Step #2: f91f7d702c35: Layer already exists\n",
            "Step #2: 77a4ffc5c2e3: Layer already exists\n",
            "Step #2: ab561809c50c: Layer already exists\n",
            "Step #2: 869f30255f10: Layer already exists\n",
            "Step #2: 2a36a760e140: Layer already exists\n",
            "Step #2: 53760c3513ea: Layer already exists\n",
            "Step #2: eacc6208b134: Layer already exists\n",
            "Step #2: 1368b6b98396: Pushed\n",
            "Step #2: c34ed7e37231: Layer already exists\n",
            "Step #2: fafe219c2252: Layer already exists\n",
            "Step #2: 3f7799709962: Layer already exists\n",
            "Step #2: 25061b98c6eb: Layer already exists\n",
            "Step #2: 2b24f8c3593f: Layer already exists\n",
            "Step #2: a548c9107c3a: Layer already exists\n",
            "Step #2: a34e40efc55d: Pushed\n",
            "Step #2: c0239f8b4c11: Pushed\n",
            "Step #2: latest: digest: sha256:4654ca59a3d49b1adcd818715ecab5b7ca6904b0dffd6244a3f59de2f4566c88 size: 4291\n",
            "Finished Step #2\n",
            "PUSH\n",
            "Pushing gcr.io/sascha-playground-doit/sentiment-fast-api\n",
            "The push refers to repository [gcr.io/sascha-playground-doit/sentiment-fast-api]\n",
            "a34e40efc55d: Preparing\n",
            "1368b6b98396: Preparing\n",
            "c0239f8b4c11: Preparing\n",
            "52da08f2bae6: Preparing\n",
            "8f574fb9904c: Preparing\n",
            "ef1a1e69814e: Preparing\n",
            "f91f7d702c35: Preparing\n",
            "ab561809c50c: Preparing\n",
            "77a4ffc5c2e3: Preparing\n",
            "869f30255f10: Preparing\n",
            "2a36a760e140: Preparing\n",
            "53760c3513ea: Preparing\n",
            "eacc6208b134: Preparing\n",
            "c34ed7e37231: Preparing\n",
            "fafe219c2252: Preparing\n",
            "3f7799709962: Preparing\n",
            "2b24f8c3593f: Preparing\n",
            "25061b98c6eb: Preparing\n",
            "a548c9107c3a: Preparing\n",
            "ef1a1e69814e: Waiting\n",
            "f91f7d702c35: Waiting\n",
            "ab561809c50c: Waiting\n",
            "77a4ffc5c2e3: Waiting\n",
            "869f30255f10: Waiting\n",
            "2a36a760e140: Waiting\n",
            "53760c3513ea: Waiting\n",
            "eacc6208b134: Waiting\n",
            "c34ed7e37231: Waiting\n",
            "fafe219c2252: Waiting\n",
            "3f7799709962: Waiting\n",
            "2b24f8c3593f: Waiting\n",
            "25061b98c6eb: Waiting\n",
            "a548c9107c3a: Waiting\n",
            "52da08f2bae6: Layer already exists\n",
            "8f574fb9904c: Layer already exists\n",
            "a34e40efc55d: Layer already exists\n",
            "1368b6b98396: Layer already exists\n",
            "c0239f8b4c11: Layer already exists\n",
            "ef1a1e69814e: Layer already exists\n",
            "f91f7d702c35: Layer already exists\n",
            "ab561809c50c: Layer already exists\n",
            "869f30255f10: Layer already exists\n",
            "77a4ffc5c2e3: Layer already exists\n",
            "53760c3513ea: Layer already exists\n",
            "eacc6208b134: Layer already exists\n",
            "c34ed7e37231: Layer already exists\n",
            "fafe219c2252: Layer already exists\n",
            "2a36a760e140: Layer already exists\n",
            "2b24f8c3593f: Layer already exists\n",
            "25061b98c6eb: Layer already exists\n",
            "3f7799709962: Layer already exists\n",
            "a548c9107c3a: Layer already exists\n",
            "latest: digest: sha256:4654ca59a3d49b1adcd818715ecab5b7ca6904b0dffd6244a3f59de2f4566c88 size: 4291\n",
            "DONE\n",
            "\n",
            "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                                IMAGES                                                      STATUS\n",
            "75c0fb6f-a11b-4552-926f-d6a5aab12ab2  2022-11-02T11:48:06+00:00  4M55S     gs://sascha-playground-doit_cloudbuild/source/1667389674.261614-ea5c988004d04af3b1e5504d1d35cd0d.tgz  gcr.io/sascha-playground-doit/sentiment-fast-api (+1 more)  SUCCESS\n"
          ]
        }
      ],
      "source": [
        "!gcloud builds submit --config cloudbuild.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload and deploy model"
      ],
      "metadata": {
        "id": "pKHyt8o7xDml"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVOxBxd7nU-w",
        "outputId": "34293ee0-6360-42d7-cca3-2c3e85cb839f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n"
          ]
        }
      ],
      "source": [
        "!gcloud ai models upload \\\n",
        "  --container-ports=80 \\\n",
        "  --container-predict-route=\"/predict\" \\\n",
        "  --container-health-route=\"/health\" \\\n",
        "  --region=us-central1 \\\n",
        "  --display-name=sentiment-fast-api \\\n",
        "  --container-image-uri=gcr.io/sascha-playground-doit/sentiment-fast-api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JG4u0OsVnhyy",
        "outputId": "b76c2d73-d45b-49aa-c961-1be853a99085"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
            "Created Vertex AI endpoint: projects/234439745674/locations/us-central1/endpoints/7608484124768075776.\n"
          ]
        }
      ],
      "source": [
        "!gcloud ai endpoints create \\\n",
        "  --project=sascha-playground-doit \\\n",
        "  --region=us-central1 \\\n",
        "  --display-name=sentiment-fast-api"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "get model and endpoint IDs from previous steps\n",
        "deployment takes 15 min aprox"
      ],
      "metadata": {
        "id": "lk_qyK39NJRq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_pme6FpoUWQ",
        "outputId": "326ff26a-358f-4d10-8f3f-a2240f62a008"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
            "Deployed a model to the endpoint 7608484124768075776. Id of the deployed model: 1352896281420234752.\n"
          ]
        }
      ],
      "source": [
        "!gcloud ai endpoints deploy-model 7608484124768075776 \\\n",
        "  --project=sascha-playground-doit \\\n",
        "  --region=us-central1 \\\n",
        "  --model=8709323962590429184 \\\n",
        "  --traffic-split=0=100 \\\n",
        "  --machine-type=\"n1-standard-2\" \\\n",
        "  --display-name=sentiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnF8bW8QXT42"
      },
      "source": [
        "## Predictions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxHBVFaBnOP8"
      },
      "source": [
        "### gcloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqQRMSltXWz9",
        "outputId": "cc542d66-4350-4e61-b337-e0a14e3277bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting request.json\n"
          ]
        }
      ],
      "source": [
        "%%writefile request.json\n",
        "{\"instances\": [\n",
        "  {\"text\":\"DoiT is a great company.\"},\n",
        "  {\"text\":\"The beach was nice but overall the hotel was very bad.\"}\n",
        "]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ygJcK3HXUw_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f14d656-e83d-4927-a246-f415130c824b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using endpoint [https://us-central1-prediction-aiplatform.googleapis.com/]\n",
            "[{'confidence': 0.9409326314926147, 'sentiment': 'POSITIVE'}, {'confidence': 0.9964427351951599, 'sentiment': 'NEGATIVE'}]\n"
          ]
        }
      ],
      "source": [
        "!gcloud ai endpoints predict 7608484124768075776 \\\n",
        "  --region=us-central1 \\\n",
        "  --json-request=request.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeDjc4SPnL1c"
      },
      "source": [
        "### Vertex AI SDK"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-cloud-aiplatform==1.14.0 --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "84yulqfiSoju",
        "outputId": "717ba75b-13f2-4e40-aa90-346a9bfd25ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting google-cloud-aiplatform==1.14.0\n",
            "  Downloading google_cloud_aiplatform-1.14.0-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 4.8 MB/s \n",
            "\u001b[?25hCollecting protobuf<4.0.0dev,>=3.19.0\n",
            "  Downloading protobuf-3.20.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 63.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-cloud-bigquery<3.0.0dev,>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-aiplatform==1.14.0) (1.21.0)\n",
            "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.7/dist-packages (from google-cloud-aiplatform==1.14.0) (1.31.6)\n",
            "Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3\n",
            "  Downloading google_cloud_resource_manager-1.6.3-py2.py3-none-any.whl (233 kB)\n",
            "\u001b[K     |████████████████████████████████| 233 kB 62.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging<22.0.0dev,>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-cloud-aiplatform==1.14.0) (21.3)\n",
            "Collecting proto-plus<2.0.0dev,>=1.15.0\n",
            "  Downloading proto_plus-1.22.1-py3-none-any.whl (47 kB)\n",
            "\u001b[K     |████████████████████████████████| 47 kB 4.6 MB/s \n",
            "\u001b[?25hCollecting google-cloud-storage<3.0.0dev,>=1.32.0\n",
            "  Downloading google_cloud_storage-2.5.0-py2.py3-none-any.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 40.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-auth<2.0dev,>=1.25.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.14.0) (1.35.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.14.0) (1.56.4)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.14.0) (1.15.0)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.14.0) (57.4.0)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.14.0) (2.23.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.14.0) (2022.5)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.29.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.14.0) (1.50.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.14.0) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.14.0) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.14.0) (4.2.4)\n",
            "Requirement already satisfied: google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.14.0) (0.4.1)\n",
            "Requirement already satisfied: google-cloud-core<2.0dev,>=1.0.3 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.14.0) (1.0.3)\n",
            "Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3\n",
            "  Downloading google_cloud_resource_manager-1.6.2-py2.py3-none-any.whl (233 kB)\n",
            "\u001b[K     |████████████████████████████████| 233 kB 41.9 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_resource_manager-1.6.1-py2.py3-none-any.whl (231 kB)\n",
            "\u001b[K     |████████████████████████████████| 231 kB 45.6 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_resource_manager-1.6.0-py2.py3-none-any.whl (231 kB)\n",
            "\u001b[K     |████████████████████████████████| 231 kB 47.2 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_resource_manager-1.5.1-py2.py3-none-any.whl (230 kB)\n",
            "\u001b[K     |████████████████████████████████| 230 kB 46.2 MB/s \n",
            "\u001b[?25hCollecting grpc-google-iam-v1<1.0.0dev,>=0.12.4\n",
            "  Downloading grpc_google_iam_v1-0.12.4-py2.py3-none-any.whl (26 kB)\n",
            "Collecting google-cloud-storage<3.0.0dev,>=1.32.0\n",
            "  Downloading google_cloud_storage-2.4.0-py2.py3-none-any.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 46.1 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-2.3.0-py2.py3-none-any.whl (107 kB)\n",
            "\u001b[K     |████████████████████████████████| 107 kB 45.3 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-2.2.1-py2.py3-none-any.whl (107 kB)\n",
            "\u001b[K     |████████████████████████████████| 107 kB 45.8 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-2.2.0-py2.py3-none-any.whl (107 kB)\n",
            "\u001b[K     |████████████████████████████████| 107 kB 45.5 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-2.1.0-py2.py3-none-any.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 48.7 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-2.0.0-py2.py3-none-any.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 59.0 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.44.0-py2.py3-none-any.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 50.4 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.43.0-py2.py3-none-any.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 55.9 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.42.3-py2.py3-none-any.whl (105 kB)\n",
            "\u001b[K     |████████████████████████████████| 105 kB 56.1 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.42.2-py2.py3-none-any.whl (105 kB)\n",
            "\u001b[K     |████████████████████████████████| 105 kB 47.6 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.42.1-py2.py3-none-any.whl (105 kB)\n",
            "\u001b[K     |████████████████████████████████| 105 kB 37.2 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.42.0-py2.py3-none-any.whl (105 kB)\n",
            "\u001b[K     |████████████████████████████████| 105 kB 45.8 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.41.1-py2.py3-none-any.whl (105 kB)\n",
            "\u001b[K     |████████████████████████████████| 105 kB 48.7 MB/s \n",
            "\u001b[?25hCollecting google-cloud-core<2.0dev,>=1.0.3\n",
            "  Downloading google_cloud_core-1.7.3-py2.py3-none-any.whl (28 kB)\n",
            "Collecting google-cloud-storage<3.0.0dev,>=1.32.0\n",
            "  Downloading google_cloud_storage-1.41.0-py2.py3-none-any.whl (104 kB)\n",
            "\u001b[K     |████████████████████████████████| 104 kB 46.3 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.40.0-py2.py3-none-any.whl (104 kB)\n",
            "\u001b[K     |████████████████████████████████| 104 kB 66.6 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.39.0-py2.py3-none-any.whl (103 kB)\n",
            "\u001b[K     |████████████████████████████████| 103 kB 67.0 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.38.0-py2.py3-none-any.whl (103 kB)\n",
            "\u001b[K     |████████████████████████████████| 103 kB 57.3 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.37.1-py2.py3-none-any.whl (103 kB)\n",
            "\u001b[K     |████████████████████████████████| 103 kB 46.1 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.37.0-py2.py3-none-any.whl (103 kB)\n",
            "\u001b[K     |████████████████████████████████| 103 kB 40.0 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.36.2-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 5.4 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.36.1-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 5.5 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.36.0-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 5.6 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.35.1-py2.py3-none-any.whl (96 kB)\n",
            "\u001b[K     |████████████████████████████████| 96 kB 5.4 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.35.0-py2.py3-none-any.whl (96 kB)\n",
            "\u001b[K     |████████████████████████████████| 96 kB 5.3 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.34.0-py2.py3-none-any.whl (96 kB)\n",
            "\u001b[K     |████████████████████████████████| 96 kB 4.0 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.33.0-py2.py3-none-any.whl (92 kB)\n",
            "\u001b[K     |████████████████████████████████| 92 kB 10.1 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_storage-1.32.0-py2.py3-none-any.whl (92 kB)\n",
            "\u001b[K     |████████████████████████████████| 92 kB 10.3 MB/s \n",
            "\u001b[?25hINFO: pip is looking at multiple versions of google-cloud-resource-manager to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3\n",
            "  Downloading google_cloud_resource_manager-1.5.0-py2.py3-none-any.whl (230 kB)\n",
            "\u001b[K     |████████████████████████████████| 230 kB 65.8 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_resource_manager-1.4.1-py2.py3-none-any.whl (402 kB)\n",
            "\u001b[K     |████████████████████████████████| 402 kB 47.7 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_resource_manager-1.4.0-py2.py3-none-any.whl (402 kB)\n",
            "\u001b[K     |████████████████████████████████| 402 kB 61.9 MB/s \n",
            "\u001b[?25h  Downloading google_cloud_resource_manager-1.3.3-py2.py3-none-any.whl (286 kB)\n",
            "\u001b[K     |████████████████████████████████| 286 kB 55.1 MB/s \n",
            "\u001b[?25hINFO: pip is looking at multiple versions of google-cloud-core to determine which version is compatible with other requirements. This could take a while.\n",
            "INFO: pip is looking at multiple versions of google-cloud-resource-manager to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting google-cloud-core<2.0dev,>=1.0.3\n",
            "  Downloading google_cloud_core-1.7.2-py2.py3-none-any.whl (28 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking\n",
            "  Downloading google_cloud_core-1.7.1-py2.py3-none-any.whl (28 kB)\n",
            "  Downloading google_cloud_core-1.7.0-py2.py3-none-any.whl (28 kB)\n",
            "  Downloading google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB)\n",
            "  Downloading google_cloud_core-1.5.0-py2.py3-none-any.whl (27 kB)\n",
            "  Downloading google_cloud_core-1.4.4-py2.py3-none-any.whl (27 kB)\n",
            "INFO: pip is looking at multiple versions of google-cloud-core to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading google_cloud_core-1.4.3-py2.py3-none-any.whl (27 kB)\n",
            "  Downloading google_cloud_core-1.4.2-py2.py3-none-any.whl (26 kB)\n",
            "  Downloading google_cloud_core-1.4.1-py2.py3-none-any.whl (26 kB)\n",
            "  Downloading google_cloud_core-1.4.0-py2.py3-none-any.whl (26 kB)\n",
            "  Downloading google_cloud_core-1.3.0-py2.py3-none-any.whl (26 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. If you want to abort this run, you can press Ctrl + C to do so. To improve how pip performs, tell us what happened here: https://pip.pypa.io/surveys/backtracking\n",
            "  Downloading google_cloud_core-1.2.0-py2.py3-none-any.whl (26 kB)\n",
            "  Downloading google_cloud_core-1.1.0-py2.py3-none-any.whl (26 kB)\n",
            "  Downloading google_cloud_core-1.0.3-py2.py3-none-any.whl (26 kB)\n",
            "INFO: pip is looking at multiple versions of google-cloud-bigquery to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting google-cloud-bigquery<3.0.0dev,>=1.15.0\n",
            "  Downloading google_cloud_bigquery-2.34.4-py2.py3-none-any.whl (206 kB)\n",
            "\u001b[K     |████████████████████████████████| 206 kB 43.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /usr/local/lib/python3.7/dist-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.14.0) (2.8.2)\n",
            "Collecting google-cloud-core<3.0.0dev,>=1.4.1\n",
            "  Downloading google_cloud_core-2.3.2-py2.py3-none-any.whl (29 kB)\n",
            "Collecting google-resumable-media<3.0dev,>=0.6.0\n",
            "  Downloading google_resumable_media-2.4.0-py2.py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 6.2 MB/s \n",
            "\u001b[?25hCollecting google-crc32c<2.0dev,>=1.0\n",
            "  Downloading google_crc32c-1.5.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging<22.0.0dev,>=14.3->google-cloud-aiplatform==1.14.0) (3.0.9)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.14.0) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.14.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.14.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.14.0) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-aiplatform==1.14.0) (2.10)\n",
            "Installing collected packages: protobuf, google-crc32c, proto-plus, grpc-google-iam-v1, google-resumable-media, google-cloud-core, google-cloud-storage, google-cloud-resource-manager, google-cloud-bigquery, google-cloud-aiplatform\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.17.3\n",
            "    Uninstalling protobuf-3.17.3:\n",
            "      Successfully uninstalled protobuf-3.17.3\n",
            "  Attempting uninstall: google-resumable-media\n",
            "    Found existing installation: google-resumable-media 0.4.1\n",
            "    Uninstalling google-resumable-media-0.4.1:\n",
            "      Successfully uninstalled google-resumable-media-0.4.1\n",
            "  Attempting uninstall: google-cloud-core\n",
            "    Found existing installation: google-cloud-core 1.0.3\n",
            "    Uninstalling google-cloud-core-1.0.3:\n",
            "      Successfully uninstalled google-cloud-core-1.0.3\n",
            "  Attempting uninstall: google-cloud-storage\n",
            "    Found existing installation: google-cloud-storage 1.18.1\n",
            "    Uninstalling google-cloud-storage-1.18.1:\n",
            "      Successfully uninstalled google-cloud-storage-1.18.1\n",
            "  Attempting uninstall: google-cloud-bigquery\n",
            "    Found existing installation: google-cloud-bigquery 1.21.0\n",
            "    Uninstalling google-cloud-bigquery-1.21.0:\n",
            "      Successfully uninstalled google-cloud-bigquery-1.21.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.9.2 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n",
            "tensorboard 2.9.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n",
            "pandas-gbq 0.13.3 requires google-cloud-bigquery[bqstorage,pandas]<2.0.0dev,>=1.11.1, but you have google-cloud-bigquery 2.34.4 which is incompatible.\n",
            "google-cloud-translate 1.5.0 requires google-cloud-core<2.0dev,>=1.0.0, but you have google-cloud-core 2.3.2 which is incompatible.\n",
            "google-cloud-firestore 1.7.0 requires google-cloud-core<2.0dev,>=1.0.3, but you have google-cloud-core 2.3.2 which is incompatible.\n",
            "google-cloud-datastore 1.8.0 requires google-cloud-core<2.0dev,>=1.0.0, but you have google-cloud-core 2.3.2 which is incompatible.\u001b[0m\n",
            "Successfully installed google-cloud-aiplatform-1.14.0 google-cloud-bigquery-2.34.4 google-cloud-core-2.3.2 google-cloud-resource-manager-1.5.1 google-cloud-storage-2.5.0 google-crc32c-1.5.0 google-resumable-media-2.4.0 grpc-google-iam-v1-0.12.4 proto-plus-1.22.1 protobuf-3.20.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import aiplatform\n",
        "\n",
        "project = 'sascha-playground-doit'\n",
        "location = 'us-central1'\n",
        "\n",
        "aiplatform.init(project=project,\n",
        "                location=location)"
      ],
      "metadata": {
        "id": "75x3lL2dSsqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krH5rUMznRQx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1da883e1-1600-47b1-bffa-d8dbb41a7fc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction(predictions=[{'sentiment': 'POSITIVE', 'confidence': 0.9409326314926147}, {'confidence': 0.9964427351951599, 'sentiment': 'NEGATIVE'}], deployed_model_id='1352896281420234752', explanations=None)\n"
          ]
        }
      ],
      "source": [
        "instances = [\n",
        "      {\n",
        "         \"text\":\"DoiT is a great company.\"\n",
        "      },\n",
        "      {\n",
        "         \"text\":\"The beach was nice but overall the hotel was very bad.\"\n",
        "      }\n",
        "   ]\n",
        "\n",
        "\n",
        "endpoint = aiplatform.Endpoint(\"projects/234439745674/locations/us-central1/endpoints/7608484124768075776\")\n",
        "\n",
        "prediction = endpoint.predict(instances=instances)\n",
        "print(prediction)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Tcaud2GWTX82"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}